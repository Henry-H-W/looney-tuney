{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fR5f4j0gghwK",
        "outputId": "8cc3f319-373c-48a2-8df2-93f3613ff5b5"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import numpy as np  # for numerical operations (arrays, matrices, etc.)\n",
        "import pandas as pd  # for handling structured data (e.g., CSV files, dataframes)\n",
        "import matplotlib.pyplot as plt  # for data visualization (graphs, charts)\n",
        "\n",
        "import os  # to handle file and directory operations\n",
        "import glob  # to find files matching a pattern (e.g., all MIDI files in a folder)\n",
        "import pickle  # to save and load serialized objects (like trained models or preprocessed data)\n",
        "import datetime # for output file specification\n",
        "\n",
        "# importing music21 - a python library for handling and analyzing music notation\n",
        "from music21 import converter, instrument, stream, note, chord\n",
        "from collections import Counter\n",
        "\n",
        "# importing deep learning tools from Keras\n",
        "from keras.models import Sequential  # for building sequential neural networks\n",
        "from keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, Flatten  # different types of layers\n",
        "from keras import utils  # utilities for handling labels, models, and training\n",
        "from keras.callbacks import ModelCheckpoint  # to save the best model during training\n",
        "from keras_self_attention import SeqSelfAttention  # self-attention mechanism for sequence models\n",
        "\n",
        "# note: ensure that you have `keras_self_attention` installed before using it.\n",
        "# you can install it using: !pip install keras-self-attention\n",
        "\n",
        "# running version 2.1.6 (assuming this is a specific requirement for compatibility)\n",
        "# check your installed version with:\n",
        "# import keras\n",
        "# print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0HR_5v1gp-d"
      },
      "outputs": [],
      "source": [
        "def train_network(notes, n_vocab):\n",
        "    \"\"\"\n",
        "    train a Neural Network to generate music based on a given sequence of notes\n",
        "\n",
        "    parameters:\n",
        "    notes (list): a list of musical notes and chords extracted from MIDI files\n",
        "    n_vocab (int): the number of unique notes/chords in the dataset (vocabulary size)\n",
        "\n",
        "    this function follows three main steps:\n",
        "    1. prepare the input sequences and corresponding output targets for the model\n",
        "    2. create the LSTM-based neural network architecture\n",
        "    3. train the model using the prepared sequences\n",
        "    \"\"\"\n",
        "\n",
        "    # step 1: convert the notes into a format that the neural network can understand\n",
        "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "    print(f\"Total training sequences generated: {len(network_input)}\")\n",
        "\n",
        "    # step 2: create the LSTM-based neural network model\n",
        "    model = create_network(network_input, n_vocab)\n",
        "\n",
        "    # step 3: train the model using the prepared input and output sequences\n",
        "    train(model, network_input, network_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CLqkbrKcgtGo"
      },
      "outputs": [],
      "source": [
        "def convert_duration(duration_value):\n",
        "    \"\"\" Converts duration to float, handling fractions like '1/3'. \"\"\"\n",
        "    try:\n",
        "        return float(duration_value)\n",
        "    except ValueError:\n",
        "        return float(Fraction(duration_value))  # Convert fraction (e.g., \"1/3\") to decimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sljaG0cIgzIr"
      },
      "outputs": [],
      "source": [
        "def adjust_octave(pitch_name, shift=-1):\n",
        "    \"\"\" Adjusts the octave of a note to fix incorrect octave shifting. \"\"\"\n",
        "    if pitch_name[-1].isdigit():  # Ensure last character is an octave number\n",
        "        note_part = pitch_name[:-1]  # Get note name (e.g., 'B')\n",
        "        octave_part = int(pitch_name[-1])  # Get octave number\n",
        "        return f\"{note_part}{octave_part + shift}\"  # Apply shift\n",
        "    return pitch_name  # If no octave detected, return as is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYsCuu7g1KM"
      },
      "outputs": [],
      "source": [
        "def get_notes():\n",
        "    \"\"\" Extracts all notes, chords, and rests from MIDI files. \"\"\"\n",
        "\n",
        "    # check if the \"data/notes\" file already exists to avoid unnecessary re-parsing\n",
        "    if os.path.exists('data/notes'):\n",
        "        print(\"skipping midi parsing - 'data/notes' already exists.\")\n",
        "        with open('data/notes', 'rb') as filepath:\n",
        "            notes = pickle.load(filepath)  # load previously parsed notes from the saved file\n",
        "        return notes  # return the existing notes data\n",
        "\n",
        "    notes = []  # Store cleaned notes, chords, and rests\n",
        "    last_offset = 0.0  # Keep track of the last note's offset\n",
        "\n",
        "    # locate all MIDI files in the dataset directory (if you have .midi files instead of .mid, edit the line below)\n",
        "    midi_files = glob.glob(\"dataset/*.mid\")\n",
        "    if not midi_files:\n",
        "        raise FileNotFoundError(\"No MIDI files found in 'dataset/' directory.\")\n",
        "\n",
        "    # iterate through all MIDI files in the dataset\n",
        "    for file in midi_files:\n",
        "        try:\n",
        "            midi = converter.parse(file)  # load MIDI file\n",
        "            print(f\"Parsing {file} ...\")\n",
        "\n",
        "            # try to extract instruments, otherwise flatten\n",
        "            try:\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse() if s2 else midi.flat.notes\n",
        "            except:\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            # iterate through each musical element and store it (note, chord, or rest)\n",
        "            for element in notes_to_parse:\n",
        "                duration_value = convert_duration(element.quarterLength)  # ensure duration is a float\n",
        "                # possible experiment: work with time (seconds) instead of note durations for absolute precision\n",
        "\n",
        "                # only add rests if there is an actual gap (this was a problem for some reason)\n",
        "                if element.offset > last_offset:\n",
        "                    rest_duration = element.offset - last_offset\n",
        "                    if rest_duration >= 0.25: \n",
        "                        notes.append(f\"rest {rest_duration}\")\n",
        "\n",
        "                if isinstance(element, note.Note):  # ðŸŽµ Single Note\n",
        "                    fixed_note = adjust_octave(element.nameWithOctave, shift=-1)\n",
        "                    notes.append(f\"{fixed_note} {duration_value}\")\n",
        "\n",
        "                elif isinstance(element, chord.Chord):  # ðŸŽ¶ Chord\n",
        "                    chord_notes = \".\".join(adjust_octave(n.nameWithOctave, shift=-1) for n in element.pitches)\n",
        "                    notes.append(f\"{chord_notes} {duration_value}\")\n",
        "\n",
        "                # update last_offset to track the most recent note's position\n",
        "                last_offset = element.offset + duration_value\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file}: {e}\")\n",
        "            continue  # continue processing other files\n",
        "\n",
        "    # apply hard cutoff: keep only the 15000 most common notes to avoid resource exhaustion errors\n",
        "    note_counts = Counter(notes)\n",
        "    most_common_notes = {note for note, _ in note_counts.most_common(15000)}\n",
        "\n",
        "    # remove rare notes entirely\n",
        "    filtered_notes = [note for note in notes if note in most_common_notes]\n",
        "\n",
        "    # save cleaned notes to a pickle file for future use\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    with open('data/notes', 'wb') as filepath:\n",
        "        pickle.dump(filtered_notes, filepath)\n",
        "\n",
        "    print(f\"Successfully extracted {len(filtered_notes)} elements from {len(midi_files)} MIDI files.\")\n",
        "    return filtered_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z5YTTbjrg8Ij"
      },
      "outputs": [],
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" prepare the sequences used by the neural network \"\"\"\n",
        "\n",
        "    sequence_length = 100  # define the length of each input sequence\n",
        "\n",
        "    # get all unique pitch names (notes, chords, and rests) and sort them\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # create a dictionary that maps each unique note/chord/rest to an integer\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    # initialize lists to store input sequences and corresponding target outputs\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and their corresponding output notes\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]  # take a sequence of 100 notes as input\n",
        "        sequence_out = notes[i + sequence_length]  # the next note after the sequence is the target output\n",
        "\n",
        "        # convert notes in the sequence to their corresponding integer values\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        # convert the target output note to its integer representation\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)  # number of training samples (patterns)\n",
        "\n",
        "    # reshape the input into a 3D format required for lstm layers: (samples, time steps, features)\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "\n",
        "    # normalize the input values to a range of 0 to 1 (helps lstm training)\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    # convert the output values into a one-hot encoded format\n",
        "    network_output = utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)  # return the processed input and output sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "syVMtLVKhC0E"
      },
      "outputs": [],
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "\n",
        "    model = Sequential()  # initialize a sequential model (a linear stack of layers)\n",
        "\n",
        "    # add a bidirectional lstm layer with 512 units\n",
        "    model.add(Bidirectional(LSTM(512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),  # shape: (time steps, features)\n",
        "        return_sequences=True)))  # return sequences to allow stacking more lstm layers\n",
        "\n",
        "    # add a self-attention layer to help the model focus on important time steps in the sequence\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "\n",
        "    # add dropout to prevent overfitting (randomly deactivates 30% of neurons)\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # add another lstm layer with 512 units, still returning sequences\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "\n",
        "    # add another dropout layer to further reduce overfitting risk\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # flatten the output before passing it to dense layers (reshapes it into a 1d vector)\n",
        "    model.add(Flatten())  # ensures compatibility with the dense output layer\n",
        "\n",
        "    # add a dense output layer with 'n_vocab' neurons (one per unique note/chord)\n",
        "    model.add(Dense(n_vocab))\n",
        "\n",
        "    # apply softmax activation to convert outputs into probabilities (multi-class classification)\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    # compile the model using categorical cross-entropy loss and rmsprop optimizer\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "    # (maybe experiment with different optimizers?)\n",
        "\n",
        "    return model  # return the compiled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "20s__pi2hSyc"
      },
      "outputs": [],
      "source": [
        "def train(model, network_input, network_output):\n",
        "    \"\"\" train the neural network \"\"\"\n",
        "\n",
        "    # let's set up a model checkpoint system that saves the model weights after every 5 epochs of training!\n",
        "    batch_size = 64\n",
        "    steps_per_epoch = len(network_input) // batch_size\n",
        "    save_freq = steps_per_epoch * 5\n",
        "\n",
        "    # Use the custom callback\n",
        "    filepath = os.path.abspath(\"weights-epoch{epoch:03d}-{loss:.4f}.keras\")\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        save_freq=save_freq, #Every 10 epochs\n",
        "        monitor='loss',\n",
        "        verbose=1,\n",
        "        save_best_only=False,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    # Then pass the callback to model.fit()\n",
        "    model.fit(network_input, network_output,\n",
        "              epochs=30,\n",
        "              batch_size=64,\n",
        "              callbacks=[checkpoint]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bvBafgc0hViI",
        "outputId": "f67ab0b0-ddbe-4bf9-f180-3935b7f84f34"
      },
      "outputs": [],
      "source": [
        "# load all musical notes, chords, and rests from midi files\n",
        "notes = get_notes()\n",
        "\n",
        "# get the total number of unique pitch names (distinct notes, chords, and rests)\n",
        "n_vocab = len(set(notes))  # converts list to set to remove duplicates, then gets its length\n",
        "\n",
        "print(f\"Vocabulary size (n_vocab): {n_vocab}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ykjLtWVYhYKQ",
        "outputId": "4281274d-d39e-4cec-fc9c-a4ac53ff401a"
      },
      "outputs": [],
      "source": [
        "# train the model using the extracted notes and the vocabulary size\n",
        "# note: before running the model, make sure you have access to a GPU!\n",
        "train_network(notes, n_vocab) # comment if you already have a weights file you want to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "weirZye3haKM"
      },
      "outputs": [],
      "source": [
        "def generate():\n",
        "    \"\"\" generate a piano midi file \"\"\"\n",
        "\n",
        "    # load the notes that were used to train the model\n",
        "    with open('data/notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)  # load the saved notes data\n",
        "\n",
        "    # get all unique pitch names (notes, chords, and rests) from the dataset\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # get the total number of unique notes (vocabulary size)\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    # prepare the input sequences for generating new music\n",
        "    network_input, normalized_input = prepare_sequences_output(notes, pitchnames, n_vocab)\n",
        "\n",
        "    # create the model and load trained weights\n",
        "    model = create_network_add_weights(normalized_input, n_vocab)\n",
        "\n",
        "    # generate a sequence of new musical notes using the trained model\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "\n",
        "    # convert the generated sequence of notes into a midi file\n",
        "    create_midi(prediction_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MZ0nvyBDhdCj"
      },
      "outputs": [],
      "source": [
        "def prepare_sequences_output(notes, pitchnames, n_vocab):\n",
        "    \"\"\" prepare the sequences used by the neural network for generating music \"\"\"\n",
        "\n",
        "    # create a dictionary to map each unique note/chord/rest to an integer\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 100  # define the length of each input sequence\n",
        "\n",
        "    # initialize lists to store input sequences and corresponding outputs\n",
        "    network_input = []\n",
        "    output = []\n",
        "\n",
        "    # create sequences of 100 notes each, using a sliding window approach\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]  # input sequence of 100 notes\n",
        "        sequence_out = notes[i + sequence_length]  # the next note (prediction target)\n",
        "\n",
        "        # convert input sequence notes to their integer representations\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "\n",
        "        # convert the output note to its corresponding integer\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)  # number of training patterns (samples)\n",
        "\n",
        "    # reshape the input into a 3d format required for lstm layers: (samples, time steps, features)\n",
        "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "\n",
        "    # normalize the input values to range 0-1 to improve model performance\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)  # return raw input and normalized input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-M72EHNhglX"
      },
      "outputs": [],
      "source": [
        "def create_network_add_weights(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network and load pre-trained weights \"\"\"\n",
        "\n",
        "    model = Sequential()  # initialize a sequential model (linear stack of layers)\n",
        "\n",
        "    # add a bidirectional lstm layer with 512 units\n",
        "    # lstm processes the input sequences while bidirectional allows learning dependencies in both directions\n",
        "    model.add(Bidirectional(LSTM(512, return_sequences=True),\n",
        "                            input_shape=(network_input.shape[1], network_input.shape[2])))\n",
        "    # input_shape must be specified in the first layer, using (time steps, features)\n",
        "\n",
        "    # add a self-attention mechanism to help the model focus on important parts of the sequence\n",
        "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "\n",
        "    # add dropout to prevent overfitting by randomly deactivating 30% of neurons\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # add another lstm layer with 512 units, still returning sequences\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "\n",
        "    # add another dropout layer to further reduce overfitting risk\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # flatten the lstm output before passing it to dense layers (reshapes into a 1d vector)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # add a dense output layer with 'n_vocab' neurons (one per unique note/chord)\n",
        "    model.add(Dense(n_vocab))\n",
        "\n",
        "    # apply softmax activation to convert outputs into probabilities (multi-class classification)\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    # compile the model using categorical cross-entropy loss (suitable for multi-class problems)\n",
        "    # rmsprop is used as the optimizer to improve training stability\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "    # load your pre-trained weights to avoid training from scratch (so change this line)\n",
        "    # this allows the model to generate music based on previously learned patterns\n",
        "    model.load_weights('weights-epoch028-3.7022.keras')\n",
        "\n",
        "    return model  # return the model with loaded weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5Ccy2fTjhj8U"
      },
      "outputs": [],
      "source": [
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\" generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = np.random.randint(0, len(network_input)-1)\n",
        "    # possible experiment: start from the end of a user-imputted midi file to 'extend' their desired song?\n",
        "\n",
        "    # create a dictionary to map integer values back to their corresponding notes\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    # get the starting sequence pattern from the input data\n",
        "    pattern = network_input[start]\n",
        "\n",
        "    # initialize an empty list to store the generated notes\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes (this controls the length of the generated music)\n",
        "    for note_index in range(500):\n",
        "        # reshape the pattern to match the lstm model's expected input shape: (samples, time steps, features)\n",
        "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\n",
        "        # normalize the input values to match the training scale\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        # get the model's prediction for the next note\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        # get the index of the highest probability note from the prediction output\n",
        "        index = np.argmax(prediction)\n",
        "        # we could also do np.random.choice(len(prediction[0]), p=prediction[0]) for more randomness\n",
        "\n",
        "        # convert the predicted index back to its corresponding note\n",
        "        result = int_to_note[index]\n",
        "\n",
        "        # store the predicted note\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        # update the input pattern by appending the new prediction and removing the first element\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]  # keep the sequence length constant\n",
        "\n",
        "    return prediction_output  # return the list of generated notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-TeTNXqhnGi"
      },
      "outputs": [],
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file \"\"\"\n",
        "\n",
        "    offset = 0  # keeps track of time to avoid overlapping notes\n",
        "    output_notes = []  # list to store the generated musical elements (notes, chords, rests)\n",
        "\n",
        "    # iterate through each predicted pattern (note, chord, or rest)\n",
        "    for pattern in prediction_output:\n",
        "        pattern = pattern.split()  # split the pattern to separate the note/chord name and duration\n",
        "        temp = pattern[0]  # extract the musical element (note, chord, or rest)\n",
        "        duration = pattern[1]  # extract the duration of the note/chord/rest\n",
        "        pattern = temp  # assign the extracted note/chord/rest back to pattern\n",
        "\n",
        "        # check if the pattern represents a chord (multiple notes played together)\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')  # split the chord into individual notes\n",
        "            notes = []  # list to store note objects\n",
        "            for current_note in notes_in_chord:\n",
        "                if current_note.isdigit():\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                else:\n",
        "                    new_note = note.Note(current_note)\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "\n",
        "        # check if the pattern represents a rest (a pause in the music)\n",
        "        elif 'rest' in pattern:\n",
        "            new_rest = note.Rest()  # create a rest without passing \"rest\" as an argument\n",
        "            new_rest.duration.quarterLength = convert_to_float(duration)  # set the duration explicitly\n",
        "            new_rest.offset = offset  # set the timing offset\n",
        "            new_rest.storedInstrument = instrument.Piano()  # assign the instrument to piano\n",
        "            output_notes.append(new_rest)  # add the rest to the output\n",
        "\n",
        "        # if the pattern is a single note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)  # create a note object\n",
        "            new_note.offset = offset  # set the timing offset\n",
        "            new_note.storedInstrument = instrument.Piano()  # assign the instrument to piano\n",
        "            output_notes.append(new_note)  # add the note to the output\n",
        "\n",
        "        # increase the offset to space out the notes and prevent stacking\n",
        "        offset += convert_to_float(duration)\n",
        "\n",
        "    # create a midi stream from the generated notes and chords\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    # write the midi stream to a file\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    midi_filename = f\"generated_music_{timestamp}.mid\"\n",
        "    midi_stream.write('midi', fp=midi_filename)\n",
        "    print(f\"Generated MIDI saved as {midi_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PLtXGsFGhuL-"
      },
      "outputs": [],
      "source": [
        "# helper function to convert fraction strings to float values\n",
        "# yes i know convert_duration and convert_to_float can be merged, i'm just too lazy to do it\n",
        "def convert_to_float(frac_str):\n",
        "    try:\n",
        "        return float(frac_str)  # try to directly convert the string to a float\n",
        "    except ValueError:  # handle cases where the string is a fraction (e.g., \"3/4\")\n",
        "        num, denom = frac_str.split('/')  # split numerator and denominator\n",
        "        try:\n",
        "            leading, num = num.split(' ')  # check for mixed fractions (e.g., \"1 3/4\")\n",
        "            whole = float(leading)  # extract the whole number part\n",
        "        except ValueError:\n",
        "            whole = 0  # if no whole number part, set to zero\n",
        "        frac = float(num) / float(denom)  # compute the fractional value\n",
        "        return whole - frac if whole < 0 else whole + frac  # return the final float value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5u_C61h5hvxn",
        "outputId": "39e58e16-18d3-41f5-ccb4-3439a7625c9f"
      },
      "outputs": [],
      "source": [
        "# run the generator to create a new midi file\n",
        "generate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
